{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 大作业：二分类问题\n",
    "#### 问题描述：\n",
    "给定video 和 sentence，判断二者的相关度\n",
    "$$\n",
    "f(v, s)=\\begin{cases}\n",
    "relevant,&res>0 \\\\ irrelevant,&otherwise\\cr\n",
    "\\end{cases}\n",
    "$$\n",
    "#### 问题思路：\n",
    "找到一个公共的空间将二者联系起来<br>\n",
    "__从sentence中提取keywords，依据名词，动词等形式分类，产生正负样本__\n",
    "#### 定义\n",
    "$V={w_0,w_1,...,w_{m-1}}$是自定义的一个字典\n",
    "\n",
    "对于一个视频样本$x$，$bow(x,i)$表示的是词$w_i$与该视频内容的相关度，$i=0,1,2,...,m-1$\n",
    "\n",
    "对于一个句子$s$，根据字典可以得到它的$bow(s)$\n",
    "\n",
    "视频与句子的相似度可通过计算对应的$bow$向量，即$bow(x)$和$bow(s)$，再由欧氏距离或余弦距离得到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定文档msrvtt10k.parser.txt和作业四中的msrvtt10k.caption.txt基本一致，不同的是video对应的句子里所有的单词都标注了词性。\n",
    "\n",
    "__因此在作业四中的一些处理也可以用于该问题(如计算video的特征向量)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msrvtt10k = \"E:\\\\msrvtt10k\\\\TextData\\\\msrvtt10k.caption.txt\"\n",
    "msrvtt10k_feature = \"E:\\\\msrvtt10k\\\\FeatureData\\\\resnext101\"\n",
    "tv2016train_A = \"E:\\\\tv2016train\\\\TextData\\\\tv2016train.setA.txt\"\n",
    "tv2016train_B = \"E:\\\\tv2016train\\\\TextData\\\\tv2016train.setB.txt\"\n",
    "tv2016train_feature = \"E:\\\\tv2016train\\\\FeatureData\\\\resnext101\"\n",
    "tv2016test_A = \"E:\\\\tv2016test\\\\TextData\\\\tv2016test.setA.txt\"\n",
    "tv2016test_B = \"E:\\\\tv2016test\\\\TextData\\\\tv2016test.setB.txt\"\n",
    "tv2016test_feature = \"E:\\\\tv2016test\\\\FeatureData\\\\resnext101\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#sklearn 自带的词频统计工具countVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = []\n",
    "a = []\n",
    "#将txt中每行句子做分隔，只保留第一个空格的后半部分\n",
    "def pick_key(x):\n",
    "    tmp = x.split(\" \",1)\n",
    "    corpus.append(tmp[1])\n",
    "    #a =[i[:i.find(\":\")] for i in tmp[1].split()]\n",
    "    #list = [' '.join(a)]\n",
    "    #print(list)\n",
    "    #corpus.append(list[0])\n",
    "    \n",
    "with open(msrvtt10k ) as p1:\n",
    "    for caption in p1.readlines():\n",
    "        caption = caption.strip('\\n')\n",
    "        pick_key(caption)\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocal = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_word(vocal, corpus):\n",
    "    des = vocal\n",
    "    for key in des:\n",
    "        des[key] = 0\n",
    "    for sentence in corpus:\n",
    "        for word in sentence.split():\n",
    "            if word in des:\n",
    "                des[word] += 1\n",
    "    return des\n",
    "\n",
    "#得到初始字典V1\n",
    "V1 = count_word(vocal, corpus)\n",
    "#print(V1)\n",
    "#去除停用词\n",
    "from nltk.corpus import stopwords\n",
    "filter_V = [word for word in V1 if word not in stopwords.words('english')]\n",
    "#print(filter_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in list(V1.keys()):\n",
    "    if key in filter_V:\n",
    "        continue\n",
    "    else:\n",
    "        del V1[key]\n",
    "#print(V1)\n",
    "#筛选频数在前5000的，得到最终字典V\n",
    "V={}\n",
    "ls = sorted(V1.items(), key=lambda d: d[1],reverse=True)\n",
    "V = dict(ls[:5000])\n",
    "#print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字典建立完毕\n",
    "\n",
    "接下来使用SVM进行模型训练和预测\n",
    "\n",
    "需要的输入有 X, y\n",
    "\n",
    "__X为样本，y为对应的label__\n",
    "\n",
    "在这个问题里，__X具体对应2048维的特征向量__，而y 则是每个视频的句子相对于字典的bow向量。<br>给出的集合里，一个video对应一个句子，所以__直接求句子的bow(s)即可。__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#找label\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "\n",
    "train_len = 200\n",
    "test_len = 1915\n",
    "feature_size = 2048\n",
    "\n",
    "#使用一个空的list sen_tmp， 用来存储每个句子的单词\n",
    "sen_tmp = []\n",
    "#label_y用来存储每个句子中是否出现字典中的词，出现为1，不出现为0\n",
    "label_y = np.zeros((train_len, len(V)),dtype=np.int)\n",
    "\n",
    "tmp_y = []\n",
    "def select_key(x):\n",
    "    tmp = x.split(\" \",1)\n",
    "    tmp_y.append(tmp[1])\n",
    "    #a =[i[:i.find(\":\")] for i in tmp[1].split()]\n",
    "    #print(a)\n",
    "    #list = [' '.join(a)]\n",
    "    #print(list)\n",
    "    #tmp_y.append(list[0])\n",
    "    \n",
    "with open(tv2016train_A ) as p1:\n",
    "    for i in p1.readlines():\n",
    "        i = i.strip('\\n')\n",
    "        select_key(i)\n",
    "        \n",
    "dict_keys = list(V.keys())\n",
    "#存字典中的key元素，为下面的匹配做准备\n",
    "#print(dict_keys)\n",
    "print(len(tmp_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对每个video找到其对应的标注：判断是否有字典V中的Key出现\n",
    "for i in range(len(tmp_y)):\n",
    "    sen_tmp = tmp_y[i].split()\n",
    "    #print(sen_tmp)\n",
    "    for j in range(0,len(V)):\n",
    "        if dict_keys[j] in sen_tmp:   \n",
    "            #print(dict_keys[j])\n",
    "            label_y[i][j] = 1\n",
    "            #句子有词出现在字典中\n",
    "        else:\n",
    "            label_y[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#找fit X \n",
    "#维数应该是200*2048\n",
    "#创建一个空的二维list，用来存储视频的特征向量(2048维)\n",
    "\n",
    "feature_train = np.zeros((train_len, feature_size), dtype=np.double)\n",
    "feature_test = np.zeros((test_len, feature_size), dtype=np.double)\n",
    "# (1915, 2048)\n",
    "\n",
    "video_train_id = []\n",
    "video_test_id = []\n",
    "\n",
    "for i in range(1, train_len+1):\n",
    "    video_train_id.append(tv2016train_A+str(i)) \n",
    "for i in range(1, test_len+1):\n",
    "    video_test_id.append(tv2016test_A+str(i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从Bigfile.py 直接复制而来\n",
    "# 用于计算train 和 test集里各个video 的2048维特征向量\n",
    "\n",
    "import os, sys, array\n",
    "\n",
    "class BigFile:\n",
    "\n",
    "    def __init__(self, datadir):\n",
    "        self.nr_of_images, self.ndims = map(int, open(os.path.join(datadir,'shape.txt')).readline().split())\n",
    "        id_file = os.path.join(datadir, \"id.txt\")\n",
    "        self.names = open(id_file).read().strip().split()\n",
    "        assert(len(self.names) == self.nr_of_images)\n",
    "        self.name2index = dict(zip(self.names, range(self.nr_of_images)))\n",
    "        self.binary_file = os.path.join(datadir, \"feature.bin\")\n",
    "        print (\"[%s] %dx%d instances loaded from %s\" % (self.__class__.__name__, self.nr_of_images, self.ndims, datadir))\n",
    "\n",
    "\n",
    "    def read(self, requested, isname=True):\n",
    "        requested = set(requested)\n",
    "        if isname:\n",
    "            index_name_array = [(self.name2index[x], x) for x in requested if x in self.name2index]\n",
    "        else:\n",
    "            assert(min(requested)>=0)\n",
    "            assert(max(requested)<len(self.names))\n",
    "            index_name_array = [(x, self.names[x]) for x in requested]\n",
    "        if len(index_name_array) == 0:\n",
    "            return [], []\n",
    "       \n",
    "        index_name_array.sort(key=lambda v:v[0])\n",
    "        sorted_index = [x[0] for x in index_name_array]\n",
    "\n",
    "        nr_of_images = len(index_name_array)\n",
    "        vecs = [None] * nr_of_images\n",
    "        offset = np.float32(1).nbytes * self.ndims\n",
    "        \n",
    "        res = array.array('f')\n",
    "        fr = open(self.binary_file, 'rb')\n",
    "        fr.seek(index_name_array[0][0] * offset)\n",
    "        res.fromfile(fr, self.ndims)\n",
    "        previous = index_name_array[0][0]\n",
    " \n",
    "        for next in sorted_index[1:]:\n",
    "            move = (next-1-previous) * offset\n",
    "            #print next, move\n",
    "            fr.seek(move, 1)\n",
    "            res.fromfile(fr, self.ndims)\n",
    "            previous = next\n",
    "\n",
    "        fr.close()\n",
    "\n",
    "        return [x[1] for x in index_name_array], [ res[i*self.ndims:(i+1)*self.ndims].tolist() for i in range(nr_of_images) ]\n",
    "\n",
    "\n",
    "    def read_one(self, name):\n",
    "        renamed, vectors = self.read([name])\n",
    "        return vectors[0]    \n",
    "\n",
    "    def shape(self):\n",
    "        return [self.nr_of_images, self.ndims]\n",
    "\n",
    "\n",
    "class StreamFile:\n",
    "\n",
    "    def __init__(self, datadir):\n",
    "        self.feat_dir = datadir\n",
    "        self.nr_of_images, self.ndims = map(int, open(os.path.join(datadir,'shape.txt')).readline().split())\n",
    "        id_file = os.path.join(datadir, \"id.txt\")\n",
    "        self.names = open(id_file).read().strip().split()\n",
    "        assert(len(self.names) == self.nr_of_images)\n",
    "        self.name2index = dict(zip(self.names, range(self.nr_of_images)))\n",
    "        self.binary_file = os.path.join(datadir, \"feature.bin\")\n",
    "        print (\"[%s] %dx%d instances loaded from %s\" % (self.__class__.__name__, self.nr_of_images, self.ndims, datadir))\n",
    "        self.fr = None\n",
    "        self.current = 0\n",
    "    \n",
    "    def open(self):\n",
    "        self.fr = open(os.path.join(self.feat_dir,'feature.bin'), 'rb')\n",
    "        self.current = 0\n",
    "\n",
    "    def close(self):\n",
    "        if self.fr:\n",
    "            self.fr.close()\n",
    "            self.fr = None\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "        \n",
    "    def next(self):\n",
    "        if self.current >= self.nr_of_images:\n",
    "            self.close()\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            res = array.array('f')\n",
    "            res.fromfile(self.fr, self.ndims)\n",
    "            _id = self.names[self.current]\n",
    "            self.current += 1\n",
    "            return _id, res.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BigFile] 200x2048 instances loaded from E:\\tv2016train\\FeatureData\\resnext101\n",
      "[BigFile] 1915x2048 instances loaded from E:\\tv2016test\\FeatureData\\resnext101\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    bigfile = BigFile(tv2016train_feature)\n",
    "    for i in range(0, train_len):\n",
    "        imset = [video_train_id[i]]\n",
    "        renamed, vectors = bigfile.read(imset)\n",
    "        j = 0\n",
    "        for name,vec in zip(renamed, vectors):\n",
    "            feature_train[i] = vec\n",
    "            j = j+1\n",
    "    #print(feature_train)\n",
    "    #训练集的特征向量\n",
    "    \n",
    "    bigfile = BigFile(tv2016test_feature)\n",
    "    for i in range(0, test_len):\n",
    "        imset = [video_test_id[i]]\n",
    "        renamed, vectors = bigfile.read(imset)\n",
    "        j = 0\n",
    "        for name,vec in zip(renamed, vectors):\n",
    "            feature_test[i] = vec\n",
    "            j = j+1\n",
    "   #print(feature_test)\n",
    "  # 测试集的特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2048)\n",
      "(5000, 200)\n",
      "(5000, 1915)\n"
     ]
    }
   ],
   "source": [
    " from sklearn import svm\n",
    "train_x = np.array(feature_train) #放入所有样本\n",
    "train_y = np.zeros((train_len, len(V)) , dtype=np.int)\n",
    "\n",
    "test_x = np.array(feature_test)\n",
    "# (1915, 2048)\n",
    "test_y = np.zeros((test_len,len(V)), dtype=np.int)\n",
    "predict_y = np.zeros((len(V),test_len), dtype=np.int)\n",
    "\n",
    "\n",
    "train_y = label_y.T\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(predict_y.shape)\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(0, len(V)):\n",
    "    k = 0\n",
    "    for j in range(0, train_len):\n",
    "        if train_y[i][j] == 0:\n",
    "            k += 1\n",
    "    if k == train_len:\n",
    "        train_y[i][random.randint(0,train_len-1)]=1\n",
    "       # print(train_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1915)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, train_len):\n",
    "    y = np.array(train_y[i])\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(train_x, y)\n",
    "    #预测test的bow值\n",
    "    predict_y[i] = clf.predict(test_x) #基于SVM对验证集做出预测\n",
    "    #print (dict_keys[i],predict_y[i])\n",
    "    \n",
    "print(predict_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### MIR思路：\n",
    "1.对于test_set中的一个视频video_i，其有唯一对应的句子Sentence_i，计算video_i与test_set中所有句子的相似度\n",
    "\n",
    "2.排序，其本身对应的句子Sentence_i的正序排名的倒数就是该视频 i 的MIR值\n",
    "\n",
    "3.求出所有视频的MIR，取平均值\n",
    "\n",
    "备注：此处的相似度用Jaccard 距离衡量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_label 是 test_setA 中1915个video的bow(s)，每个vieo_i对应一个bow(s_i)\n",
    "# label_y 是 train_setA 中1915个video的bow(s)\n",
    "from sklearn.metrics import jaccard_similarity_score as jac\n",
    "\n",
    "MIR = np.zeros(test_len, dtype=np.double)\n",
    "tmp_mir = {}\n",
    "res_mir = 0.0\n",
    "new_sort = []\n",
    "k = 0.0\n",
    "\n",
    "yy = predict_y.T\n",
    "\n",
    "for i in range(0, test_len):\n",
    "    v1 = np.array(yy[i])\n",
    "    for j in range(0,test_len):\n",
    "        v2 = np.array(yy[j])\n",
    "        tmp_mir[j] = jac(v1,v2)\n",
    "        #计算video_i与test_set中所有句子的相似度\n",
    "    new_sort = sorted(tmp_mir.items(), key=lambda d: d[1],reverse=True)\n",
    "    #对相似度排序\n",
    "    for x in range(0, len(new_sort)):\n",
    "        if i == new_sort[x][0]:\n",
    "            x = x+1\n",
    "            k += 1/x\n",
    "            #取本身对应的句子的排名的倒数，得到video_i 的MIR\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004248015478014735\n"
     ]
    }
   ],
   "source": [
    "res_mir = k/1915\n",
    "print(res_mir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
